{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sriramthotapallim/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Custom Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = \"I am going to India . I am going to Bharat . I will be eating coffee . I will be drinking coffee .\"\n",
    "text_corpus = text_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in text_corpus.split() if word!='.']\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am',\n",
       " 'be',\n",
       " 'bharat',\n",
       " 'coffee',\n",
       " 'drinking',\n",
       " 'eating',\n",
       " 'going',\n",
       " 'i',\n",
       " 'india',\n",
       " 'to',\n",
       " 'will'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Int & Int2Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "drinking\n"
     ]
    }
   ],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "corpus_size = len(words)\n",
    "\n",
    "for index,word in enumerate(words):\n",
    "    word2int[word] = index\n",
    "    int2word[index] = word\n",
    "    \n",
    "print(word2int['india'])\n",
    "print(int2word[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'going': 0,\n",
       " 'to': 1,\n",
       " 'india': 2,\n",
       " 'i': 3,\n",
       " 'am': 4,\n",
       " 'eating': 5,\n",
       " 'coffee': 6,\n",
       " 'drinking': 7,\n",
       " 'be': 8,\n",
       " 'bharat': 9,\n",
       " 'will': 10}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'am', 'going', 'to', 'india'], ['i', 'am', 'going', 'to', 'bharat'], ['i', 'will', 'be', 'eating', 'coffee'], ['i', 'will', 'be', 'drinking', 'coffee']]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "[sentences.append(sentence.split()) for sentence in text_corpus.split('.') if sentence]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the context words for a given target word of window size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'am'],\n",
       " ['i', 'going'],\n",
       " ['am', 'i'],\n",
       " ['am', 'going'],\n",
       " ['am', 'to'],\n",
       " ['going', 'i'],\n",
       " ['going', 'am'],\n",
       " ['going', 'to'],\n",
       " ['going', 'india'],\n",
       " ['to', 'am'],\n",
       " ['to', 'going'],\n",
       " ['to', 'india'],\n",
       " ['india', 'going'],\n",
       " ['india', 'to'],\n",
       " ['i', 'am'],\n",
       " ['i', 'going'],\n",
       " ['am', 'i'],\n",
       " ['am', 'going'],\n",
       " ['am', 'to'],\n",
       " ['going', 'i'],\n",
       " ['going', 'am'],\n",
       " ['going', 'to'],\n",
       " ['going', 'bharat'],\n",
       " ['to', 'am'],\n",
       " ['to', 'going'],\n",
       " ['to', 'bharat'],\n",
       " ['bharat', 'going'],\n",
       " ['bharat', 'to'],\n",
       " ['i', 'will'],\n",
       " ['i', 'be'],\n",
       " ['will', 'i'],\n",
       " ['will', 'be'],\n",
       " ['will', 'eating'],\n",
       " ['be', 'i'],\n",
       " ['be', 'will'],\n",
       " ['be', 'eating'],\n",
       " ['be', 'coffee'],\n",
       " ['eating', 'will'],\n",
       " ['eating', 'be'],\n",
       " ['eating', 'coffee'],\n",
       " ['coffee', 'be'],\n",
       " ['coffee', 'eating'],\n",
       " ['i', 'will'],\n",
       " ['i', 'be'],\n",
       " ['will', 'i'],\n",
       " ['will', 'be'],\n",
       " ['will', 'drinking'],\n",
       " ['be', 'i'],\n",
       " ['be', 'will'],\n",
       " ['be', 'drinking'],\n",
       " ['be', 'coffee'],\n",
       " ['drinking', 'will'],\n",
       " ['drinking', 'be'],\n",
       " ['drinking', 'coffee'],\n",
       " ['coffee', 'be'],\n",
       " ['coffee', 'drinking']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 2\n",
    "final_data = []\n",
    "\n",
    "for sent in sentences:\n",
    "    for index,word in enumerate(sent):\n",
    "        for context_word in sent[max(index - window_size,0) : min(index + window_size, len(sent)) + 1]:\n",
    "            if context_word != word:\n",
    "                final_data.append([word,context_word])\n",
    "\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word_index,size):\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[word_index] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [] \n",
    "y_train = [] \n",
    "\n",
    "for words in final_data:\n",
    "    x_train.append(one_hot_encoding(word2int[words[0]],corpus_size))\n",
    "    y_train.append(one_hot_encoding(word2int[words[1]],corpus_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Neural Network model and Training the data using Cross Entropy Loss as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = tf.placeholder(tf.float32, shape=(None, corpus_size))\n",
    "y_data = tf.placeholder(tf.float32, shape=(None, corpus_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 5\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([corpus_size, embedding_size]))\n",
    "b1 = tf.Variable(tf.random_normal([embedding_size]))\n",
    "hidden_weights = tf.add(tf.matmul(x_data,w1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = tf.Variable(tf.random_normal([embedding_size, corpus_size]))\n",
    "\n",
    "b2 = tf.Variable(tf.random_normal([corpus_size]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_weights, w2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is :  4.4782166\n",
      "Loss is :  4.1746984\n",
      "Loss is :  3.9502368\n",
      "Loss is :  3.776115\n",
      "Loss is :  3.6343653\n",
      "Loss is :  3.5150752\n",
      "Loss is :  3.4126208\n",
      "Loss is :  3.3234692\n",
      "Loss is :  3.2451377\n",
      "Loss is :  3.1757252\n",
      "Loss is :  3.113708\n",
      "Loss is :  3.0578396\n",
      "Loss is :  3.0071018\n",
      "Loss is :  2.9606655\n",
      "Loss is :  2.9178581\n",
      "Loss is :  2.8781388\n",
      "Loss is :  2.8410704\n",
      "Loss is :  2.806301\n",
      "Loss is :  2.773544\n",
      "Loss is :  2.7425656\n",
      "Loss is :  2.7131734\n",
      "Loss is :  2.6852067\n",
      "Loss is :  2.6585307\n",
      "Loss is :  2.6330316\n",
      "Loss is :  2.608612\n",
      "Loss is :  2.585188\n",
      "Loss is :  2.5626855\n",
      "Loss is :  2.5410411\n",
      "Loss is :  2.5201976\n",
      "Loss is :  2.5001054\n",
      "Loss is :  2.4807184\n",
      "Loss is :  2.4619968\n",
      "Loss is :  2.4439034\n",
      "Loss is :  2.426405\n",
      "Loss is :  2.409471\n",
      "Loss is :  2.3930728\n",
      "Loss is :  2.3771853\n",
      "Loss is :  2.361784\n",
      "Loss is :  2.346846\n",
      "Loss is :  2.332352\n",
      "Loss is :  2.318281\n",
      "Loss is :  2.304615\n",
      "Loss is :  2.2913375\n",
      "Loss is :  2.2784314\n",
      "Loss is :  2.2658823\n",
      "Loss is :  2.253675\n",
      "Loss is :  2.241796\n",
      "Loss is :  2.2302327\n",
      "Loss is :  2.2189724\n",
      "Loss is :  2.2080035\n",
      "Loss is :  2.1973147\n",
      "Loss is :  2.1868958\n",
      "Loss is :  2.1767366\n",
      "Loss is :  2.166827\n",
      "Loss is :  2.1571586\n",
      "Loss is :  2.1477225\n",
      "Loss is :  2.13851\n",
      "Loss is :  2.1295135\n",
      "Loss is :  2.1207254\n",
      "Loss is :  2.1121383\n",
      "Loss is :  2.1037452\n",
      "Loss is :  2.0955398\n",
      "Loss is :  2.0875154\n",
      "Loss is :  2.0796664\n",
      "Loss is :  2.0719867\n",
      "Loss is :  2.0644705\n",
      "Loss is :  2.0571132\n",
      "Loss is :  2.049909\n",
      "Loss is :  2.0428534\n",
      "Loss is :  2.0359418\n",
      "Loss is :  2.0291693\n",
      "Loss is :  2.0225322\n",
      "Loss is :  2.016026\n",
      "Loss is :  2.0096471\n",
      "Loss is :  2.0033913\n",
      "Loss is :  1.9972551\n",
      "Loss is :  1.9912351\n",
      "Loss is :  1.9853281\n",
      "Loss is :  1.9795309\n",
      "Loss is :  1.9738396\n",
      "Loss is :  1.9682524\n",
      "Loss is :  1.962766\n",
      "Loss is :  1.9573776\n",
      "Loss is :  1.9520844\n",
      "Loss is :  1.9468844\n",
      "Loss is :  1.9417746\n",
      "Loss is :  1.936753\n",
      "Loss is :  1.9318172\n",
      "Loss is :  1.9269649\n",
      "Loss is :  1.9221945\n",
      "Loss is :  1.9175036\n",
      "Loss is :  1.9128903\n",
      "Loss is :  1.9083529\n",
      "Loss is :  1.9038891\n",
      "Loss is :  1.8994977\n",
      "Loss is :  1.8951768\n",
      "Loss is :  1.8909247\n",
      "Loss is :  1.8867399\n",
      "Loss is :  1.8826209\n",
      "Loss is :  1.8785661\n",
      "Loss is :  1.8745744\n",
      "Loss is :  1.8706437\n",
      "Loss is :  1.8667735\n",
      "Loss is :  1.8629618\n",
      "Loss is :  1.8592077\n",
      "Loss is :  1.8555099\n",
      "Loss is :  1.8518673\n",
      "Loss is :  1.8482783\n",
      "Loss is :  1.8447424\n",
      "Loss is :  1.8412579\n",
      "Loss is :  1.8378241\n",
      "Loss is :  1.83444\n",
      "Loss is :  1.8311042\n",
      "Loss is :  1.827816\n",
      "Loss is :  1.8245744\n",
      "Loss is :  1.8213781\n",
      "Loss is :  1.8182268\n",
      "Loss is :  1.8151191\n",
      "Loss is :  1.8120544\n",
      "Loss is :  1.8090318\n",
      "Loss is :  1.8060502\n",
      "Loss is :  1.803109\n",
      "Loss is :  1.8002071\n",
      "Loss is :  1.7973441\n",
      "Loss is :  1.7945192\n",
      "Loss is :  1.7917311\n",
      "Loss is :  1.7889799\n",
      "Loss is :  1.7862642\n",
      "Loss is :  1.7835834\n",
      "Loss is :  1.780937\n",
      "Loss is :  1.7783244\n",
      "Loss is :  1.7757446\n",
      "Loss is :  1.7731972\n",
      "Loss is :  1.7706814\n",
      "Loss is :  1.7681968\n",
      "Loss is :  1.7657424\n",
      "Loss is :  1.7633182\n",
      "Loss is :  1.760923\n",
      "Loss is :  1.7585565\n",
      "Loss is :  1.7562182\n",
      "Loss is :  1.7539076\n",
      "Loss is :  1.7516237\n",
      "Loss is :  1.7493665\n",
      "Loss is :  1.7471353\n",
      "Loss is :  1.7449297\n",
      "Loss is :  1.7427489\n",
      "Loss is :  1.7405927\n",
      "Loss is :  1.7384605\n",
      "Loss is :  1.7363521\n",
      "Loss is :  1.7342666\n",
      "Loss is :  1.7322041\n",
      "Loss is :  1.7301636\n",
      "Loss is :  1.728145\n",
      "Loss is :  1.726148\n",
      "Loss is :  1.724172\n",
      "Loss is :  1.7222166\n",
      "Loss is :  1.7202817\n",
      "Loss is :  1.7183666\n",
      "Loss is :  1.716471\n",
      "Loss is :  1.7145945\n",
      "Loss is :  1.712737\n",
      "Loss is :  1.710898\n",
      "Loss is :  1.7090771\n",
      "Loss is :  1.7072741\n",
      "Loss is :  1.7054884\n",
      "Loss is :  1.7037203\n",
      "Loss is :  1.701969\n",
      "Loss is :  1.7002343\n",
      "Loss is :  1.698516\n",
      "Loss is :  1.6968136\n",
      "Loss is :  1.6951271\n",
      "Loss is :  1.693456\n",
      "Loss is :  1.6918002\n",
      "Loss is :  1.6901596\n",
      "Loss is :  1.6885335\n",
      "Loss is :  1.6869221\n",
      "Loss is :  1.6853248\n",
      "Loss is :  1.6837415\n",
      "Loss is :  1.6821721\n",
      "Loss is :  1.6806161\n",
      "Loss is :  1.6790736\n",
      "Loss is :  1.6775442\n",
      "Loss is :  1.6760279\n",
      "Loss is :  1.6745241\n",
      "Loss is :  1.6730328\n",
      "Loss is :  1.671554\n",
      "Loss is :  1.6700871\n",
      "Loss is :  1.6686324\n",
      "Loss is :  1.6671892\n",
      "Loss is :  1.6657579\n",
      "Loss is :  1.6643375\n",
      "Loss is :  1.6629288\n",
      "Loss is :  1.6615311\n",
      "Loss is :  1.6601442\n",
      "Loss is :  1.6587679\n",
      "Loss is :  1.6574023\n",
      "Loss is :  1.6560472\n",
      "Loss is :  1.6547022\n",
      "Loss is :  1.6533674\n",
      "Loss is :  1.6520425\n",
      "Loss is :  1.6507275\n",
      "Loss is :  1.6494223\n",
      "Loss is :  1.6481264\n",
      "Loss is :  1.64684\n",
      "Loss is :  1.6455629\n",
      "Loss is :  1.6442949\n",
      "Loss is :  1.6430359\n",
      "Loss is :  1.6417857\n",
      "Loss is :  1.6405445\n",
      "Loss is :  1.6393117\n",
      "Loss is :  1.6380876\n",
      "Loss is :  1.6368719\n",
      "Loss is :  1.6356646\n",
      "Loss is :  1.6344652\n",
      "Loss is :  1.633274\n",
      "Loss is :  1.6320908\n",
      "Loss is :  1.6309153\n",
      "Loss is :  1.6297476\n",
      "Loss is :  1.6285877\n",
      "Loss is :  1.6274353\n",
      "Loss is :  1.6262902\n",
      "Loss is :  1.6251527\n",
      "Loss is :  1.6240224\n",
      "Loss is :  1.6228992\n",
      "Loss is :  1.621783\n",
      "Loss is :  1.6206739\n",
      "Loss is :  1.6195716\n",
      "Loss is :  1.618476\n",
      "Loss is :  1.6173872\n",
      "Loss is :  1.6163052\n",
      "Loss is :  1.6152297\n",
      "Loss is :  1.6141605\n",
      "Loss is :  1.613098\n",
      "Loss is :  1.6120415\n",
      "Loss is :  1.6109914\n",
      "Loss is :  1.6099474\n",
      "Loss is :  1.6089095\n",
      "Loss is :  1.6078777\n",
      "Loss is :  1.6068516\n",
      "Loss is :  1.6058315\n",
      "Loss is :  1.6048173\n",
      "Loss is :  1.6038086\n",
      "Loss is :  1.6028059\n",
      "Loss is :  1.6018085\n",
      "Loss is :  1.6008167\n",
      "Loss is :  1.5998305\n",
      "Loss is :  1.5988495\n",
      "Loss is :  1.597874\n",
      "Loss is :  1.5969037\n",
      "Loss is :  1.5959388\n",
      "Loss is :  1.5949789\n",
      "Loss is :  1.5940241\n",
      "Loss is :  1.5930742\n",
      "Loss is :  1.5921296\n",
      "Loss is :  1.5911897\n",
      "Loss is :  1.5902549\n",
      "Loss is :  1.5893247\n",
      "Loss is :  1.5883993\n",
      "Loss is :  1.5874789\n",
      "Loss is :  1.5865629\n",
      "Loss is :  1.5856514\n",
      "Loss is :  1.5847447\n",
      "Loss is :  1.5838425\n",
      "Loss is :  1.5829446\n",
      "Loss is :  1.5820514\n",
      "Loss is :  1.5811626\n",
      "Loss is :  1.5802778\n",
      "Loss is :  1.5793976\n",
      "Loss is :  1.5785216\n",
      "Loss is :  1.5776498\n",
      "Loss is :  1.576782\n",
      "Loss is :  1.5759184\n",
      "Loss is :  1.5750589\n",
      "Loss is :  1.5742036\n",
      "Loss is :  1.573352\n",
      "Loss is :  1.5725046\n",
      "Loss is :  1.5716611\n",
      "Loss is :  1.5708214\n",
      "Loss is :  1.5699855\n",
      "Loss is :  1.5691535\n",
      "Loss is :  1.5683253\n",
      "Loss is :  1.5675008\n",
      "Loss is :  1.56668\n",
      "Loss is :  1.5658628\n",
      "Loss is :  1.5650493\n",
      "Loss is :  1.5642395\n",
      "Loss is :  1.5634329\n",
      "Loss is :  1.5626303\n",
      "Loss is :  1.5618309\n",
      "Loss is :  1.561035\n",
      "Loss is :  1.5602427\n",
      "Loss is :  1.5594537\n",
      "Loss is :  1.558668\n",
      "Loss is :  1.5578859\n",
      "Loss is :  1.5571069\n",
      "Loss is :  1.556331\n",
      "Loss is :  1.5555589\n",
      "Loss is :  1.5547897\n",
      "Loss is :  1.5540237\n",
      "Loss is :  1.5532609\n",
      "Loss is :  1.5525013\n",
      "Loss is :  1.5517448\n",
      "Loss is :  1.5509914\n",
      "Loss is :  1.5502412\n",
      "Loss is :  1.5494939\n",
      "Loss is :  1.5487497\n",
      "Loss is :  1.5480086\n",
      "Loss is :  1.5472704\n",
      "Loss is :  1.5465349\n",
      "Loss is :  1.5458025\n",
      "Loss is :  1.5450732\n",
      "Loss is :  1.5443466\n",
      "Loss is :  1.5436229\n",
      "Loss is :  1.542902\n",
      "Loss is :  1.542184\n",
      "Loss is :  1.5414687\n",
      "Loss is :  1.5407562\n",
      "Loss is :  1.5400465\n",
      "Loss is :  1.5393397\n",
      "Loss is :  1.5386354\n",
      "Loss is :  1.537934\n",
      "Loss is :  1.537235\n",
      "Loss is :  1.5365387\n",
      "Loss is :  1.5358452\n",
      "Loss is :  1.5351541\n",
      "Loss is :  1.5344658\n",
      "Loss is :  1.53378\n",
      "Loss is :  1.5330967\n",
      "Loss is :  1.5324162\n",
      "Loss is :  1.531738\n",
      "Loss is :  1.5310625\n",
      "Loss is :  1.5303892\n",
      "Loss is :  1.5297186\n",
      "Loss is :  1.5290506\n",
      "Loss is :  1.5283848\n",
      "Loss is :  1.5277214\n",
      "Loss is :  1.5270606\n",
      "Loss is :  1.5264021\n",
      "Loss is :  1.5257461\n",
      "Loss is :  1.5250922\n",
      "Loss is :  1.5244409\n",
      "Loss is :  1.5237919\n",
      "Loss is :  1.5231453\n",
      "Loss is :  1.5225009\n",
      "Loss is :  1.5218588\n",
      "Loss is :  1.521219\n",
      "Loss is :  1.5205814\n",
      "Loss is :  1.5199461\n",
      "Loss is :  1.5193131\n",
      "Loss is :  1.5186822\n",
      "Loss is :  1.5180538\n",
      "Loss is :  1.5174273\n",
      "Loss is :  1.516803\n",
      "Loss is :  1.5161811\n",
      "Loss is :  1.5155611\n",
      "Loss is :  1.5149434\n",
      "Loss is :  1.5143279\n",
      "Loss is :  1.5137146\n",
      "Loss is :  1.5131032\n",
      "Loss is :  1.512494\n",
      "Loss is :  1.5118868\n",
      "Loss is :  1.5112817\n",
      "Loss is :  1.5106786\n",
      "Loss is :  1.5100778\n",
      "Loss is :  1.5094789\n",
      "Loss is :  1.5088823\n",
      "Loss is :  1.5082874\n",
      "Loss is :  1.5076948\n",
      "Loss is :  1.5071038\n",
      "Loss is :  1.5065151\n",
      "Loss is :  1.5059283\n",
      "Loss is :  1.5053434\n",
      "Loss is :  1.5047605\n",
      "Loss is :  1.5041798\n",
      "Loss is :  1.5036008\n",
      "Loss is :  1.5030237\n",
      "Loss is :  1.5024488\n",
      "Loss is :  1.5018755\n",
      "Loss is :  1.5013043\n",
      "Loss is :  1.5007347\n",
      "Loss is :  1.5001673\n",
      "Loss is :  1.4996016\n",
      "Loss is :  1.4990381\n",
      "Loss is :  1.4984761\n",
      "Loss is :  1.4979161\n",
      "Loss is :  1.4973581\n",
      "Loss is :  1.4968016\n",
      "Loss is :  1.4962472\n",
      "Loss is :  1.4956944\n",
      "Loss is :  1.4951435\n",
      "Loss is :  1.4945946\n",
      "Loss is :  1.4940472\n",
      "Loss is :  1.4935018\n",
      "Loss is :  1.4929581\n",
      "Loss is :  1.4924161\n",
      "Loss is :  1.4918758\n",
      "Loss is :  1.4913374\n",
      "Loss is :  1.4908007\n",
      "Loss is :  1.4902658\n",
      "Loss is :  1.4897326\n",
      "Loss is :  1.4892012\n",
      "Loss is :  1.4886714\n",
      "Loss is :  1.4881432\n",
      "Loss is :  1.4876169\n",
      "Loss is :  1.4870923\n",
      "Loss is :  1.4865693\n",
      "Loss is :  1.486048\n",
      "Loss is :  1.4855283\n",
      "Loss is :  1.4850104\n",
      "Loss is :  1.4844941\n",
      "Loss is :  1.4839793\n",
      "Loss is :  1.4834664\n",
      "Loss is :  1.4829551\n",
      "Loss is :  1.4824454\n",
      "Loss is :  1.4819372\n",
      "Loss is :  1.4814308\n",
      "Loss is :  1.4809258\n",
      "Loss is :  1.4804227\n",
      "Loss is :  1.4799211\n",
      "Loss is :  1.4794208\n",
      "Loss is :  1.4789225\n",
      "Loss is :  1.4784254\n",
      "Loss is :  1.4779301\n",
      "Loss is :  1.4774364\n",
      "Loss is :  1.4769442\n",
      "Loss is :  1.4764535\n",
      "Loss is :  1.4759644\n",
      "Loss is :  1.4754769\n",
      "Loss is :  1.474991\n",
      "Loss is :  1.4745064\n",
      "Loss is :  1.4740236\n",
      "Loss is :  1.4735421\n",
      "Loss is :  1.4730622\n",
      "Loss is :  1.4725838\n",
      "Loss is :  1.4721068\n",
      "Loss is :  1.4716314\n",
      "Loss is :  1.4711574\n",
      "Loss is :  1.470685\n",
      "Loss is :  1.470214\n",
      "Loss is :  1.4697446\n",
      "Loss is :  1.4692767\n",
      "Loss is :  1.4688101\n",
      "Loss is :  1.4683448\n",
      "Loss is :  1.4678813\n",
      "Loss is :  1.4674193\n",
      "Loss is :  1.4669584\n",
      "Loss is :  1.4664992\n",
      "Loss is :  1.4660413\n",
      "Loss is :  1.4655848\n",
      "Loss is :  1.4651299\n",
      "Loss is :  1.4646761\n",
      "Loss is :  1.464224\n",
      "Loss is :  1.4637734\n",
      "Loss is :  1.4633238\n",
      "Loss is :  1.462876\n",
      "Loss is :  1.4624293\n",
      "Loss is :  1.461984\n",
      "Loss is :  1.4615402\n",
      "Loss is :  1.4610977\n",
      "Loss is :  1.4606568\n",
      "Loss is :  1.4602171\n",
      "Loss is :  1.4597787\n",
      "Loss is :  1.4593418\n",
      "Loss is :  1.4589062\n",
      "Loss is :  1.4584719\n",
      "Loss is :  1.4580389\n",
      "Loss is :  1.4576072\n",
      "Loss is :  1.457177\n",
      "Loss is :  1.456748\n",
      "Loss is :  1.4563204\n",
      "Loss is :  1.4558941\n",
      "Loss is :  1.4554691\n",
      "Loss is :  1.4550455\n",
      "Loss is :  1.4546231\n",
      "Loss is :  1.4542019\n",
      "Loss is :  1.4537822\n",
      "Loss is :  1.4533637\n",
      "Loss is :  1.4529467\n",
      "Loss is :  1.4525305\n",
      "Loss is :  1.452116\n",
      "Loss is :  1.4517027\n",
      "Loss is :  1.4512905\n",
      "Loss is :  1.4508797\n",
      "Loss is :  1.4504702\n",
      "Loss is :  1.4500618\n",
      "Loss is :  1.4496547\n",
      "Loss is :  1.449249\n",
      "Loss is :  1.4488443\n",
      "Loss is :  1.4484411\n",
      "Loss is :  1.448039\n",
      "Loss is :  1.4476382\n",
      "Loss is :  1.4472383\n",
      "Loss is :  1.4468402\n",
      "Loss is :  1.446443\n",
      "Loss is :  1.4460468\n",
      "Loss is :  1.4456521\n",
      "Loss is :  1.4452585\n",
      "Loss is :  1.4448663\n",
      "Loss is :  1.4444752\n",
      "Loss is :  1.4440852\n",
      "Loss is :  1.4436963\n",
      "Loss is :  1.4433087\n",
      "Loss is :  1.4429224\n",
      "Loss is :  1.4425371\n",
      "Loss is :  1.4421531\n",
      "Loss is :  1.4417703\n",
      "Loss is :  1.4413884\n",
      "Loss is :  1.4410079\n",
      "Loss is :  1.4406284\n",
      "Loss is :  1.4402502\n",
      "Loss is :  1.4398731\n",
      "Loss is :  1.4394972\n",
      "Loss is :  1.4391223\n",
      "Loss is :  1.4387487\n",
      "Loss is :  1.4383762\n",
      "Loss is :  1.4380046\n",
      "Loss is :  1.4376343\n",
      "Loss is :  1.4372653\n",
      "Loss is :  1.4368973\n",
      "Loss is :  1.4365304\n",
      "Loss is :  1.4361645\n",
      "Loss is :  1.4357998\n",
      "Loss is :  1.4354362\n",
      "Loss is :  1.4350737\n",
      "Loss is :  1.4347123\n",
      "Loss is :  1.434352\n",
      "Loss is :  1.4339927\n",
      "Loss is :  1.4336346\n",
      "Loss is :  1.4332777\n",
      "Loss is :  1.4329215\n",
      "Loss is :  1.4325665\n",
      "Loss is :  1.4322128\n",
      "Loss is :  1.43186\n",
      "Loss is :  1.4315083\n",
      "Loss is :  1.4311577\n",
      "Loss is :  1.4308082\n",
      "Loss is :  1.4304596\n",
      "Loss is :  1.430112\n",
      "Loss is :  1.4297656\n",
      "Loss is :  1.4294202\n",
      "Loss is :  1.4290758\n",
      "Loss is :  1.4287326\n",
      "Loss is :  1.4283901\n",
      "Loss is :  1.428049\n",
      "Loss is :  1.4277087\n",
      "Loss is :  1.4273694\n",
      "Loss is :  1.4270313\n",
      "Loss is :  1.426694\n",
      "Loss is :  1.4263579\n",
      "Loss is :  1.4260226\n",
      "Loss is :  1.4256886\n",
      "Loss is :  1.4253553\n",
      "Loss is :  1.4250232\n",
      "Loss is :  1.4246922\n",
      "Loss is :  1.4243618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is :  1.4240328\n",
      "Loss is :  1.4237045\n",
      "Loss is :  1.4233772\n",
      "Loss is :  1.423051\n",
      "Loss is :  1.4227257\n",
      "Loss is :  1.4224014\n",
      "Loss is :  1.422078\n",
      "Loss is :  1.4217557\n",
      "Loss is :  1.4214342\n",
      "Loss is :  1.4211137\n",
      "Loss is :  1.4207944\n",
      "Loss is :  1.4204757\n",
      "Loss is :  1.420158\n",
      "Loss is :  1.4198414\n",
      "Loss is :  1.4195257\n",
      "Loss is :  1.419211\n",
      "Loss is :  1.418897\n",
      "Loss is :  1.4185841\n",
      "Loss is :  1.4182721\n",
      "Loss is :  1.417961\n",
      "Loss is :  1.4176509\n",
      "Loss is :  1.4173418\n",
      "Loss is :  1.4170333\n",
      "Loss is :  1.4167261\n",
      "Loss is :  1.4164194\n",
      "Loss is :  1.416114\n",
      "Loss is :  1.4158094\n",
      "Loss is :  1.4155055\n",
      "Loss is :  1.4152027\n",
      "Loss is :  1.4149007\n",
      "Loss is :  1.4145995\n",
      "Loss is :  1.4142992\n",
      "Loss is :  1.4140002\n",
      "Loss is :  1.4137018\n",
      "Loss is :  1.4134042\n",
      "Loss is :  1.4131075\n",
      "Loss is :  1.4128116\n",
      "Loss is :  1.4125167\n",
      "Loss is :  1.4122227\n",
      "Loss is :  1.4119295\n",
      "Loss is :  1.4116372\n",
      "Loss is :  1.4113457\n",
      "Loss is :  1.4110552\n",
      "Loss is :  1.4107654\n",
      "Loss is :  1.4104764\n",
      "Loss is :  1.4101886\n",
      "Loss is :  1.4099013\n",
      "Loss is :  1.4096148\n",
      "Loss is :  1.4093293\n",
      "Loss is :  1.4090446\n",
      "Loss is :  1.4087608\n",
      "Loss is :  1.4084778\n",
      "Loss is :  1.4081956\n",
      "Loss is :  1.4079142\n",
      "Loss is :  1.4076337\n",
      "Loss is :  1.4073538\n",
      "Loss is :  1.4070749\n",
      "Loss is :  1.4067968\n",
      "Loss is :  1.4065195\n",
      "Loss is :  1.4062432\n",
      "Loss is :  1.4059675\n",
      "Loss is :  1.4056925\n",
      "Loss is :  1.4054184\n",
      "Loss is :  1.4051453\n",
      "Loss is :  1.4048729\n",
      "Loss is :  1.4046011\n",
      "Loss is :  1.4043301\n",
      "Loss is :  1.4040601\n",
      "Loss is :  1.4037907\n",
      "Loss is :  1.4035223\n",
      "Loss is :  1.4032544\n",
      "Loss is :  1.4029875\n",
      "Loss is :  1.4027213\n",
      "Loss is :  1.4024557\n",
      "Loss is :  1.4021912\n",
      "Loss is :  1.4019272\n",
      "Loss is :  1.4016641\n",
      "Loss is :  1.4014018\n",
      "Loss is :  1.4011402\n",
      "Loss is :  1.4008793\n",
      "Loss is :  1.4006193\n",
      "Loss is :  1.4003599\n",
      "Loss is :  1.4001013\n",
      "Loss is :  1.3998435\n",
      "Loss is :  1.3995863\n",
      "Loss is :  1.3993299\n",
      "Loss is :  1.3990743\n",
      "Loss is :  1.3988192\n",
      "Loss is :  1.3985653\n",
      "Loss is :  1.3983117\n",
      "Loss is :  1.398059\n",
      "Loss is :  1.3978071\n",
      "Loss is :  1.397556\n",
      "Loss is :  1.3973054\n",
      "Loss is :  1.3970556\n",
      "Loss is :  1.3968065\n",
      "Loss is :  1.396558\n",
      "Loss is :  1.3963106\n",
      "Loss is :  1.3960634\n",
      "Loss is :  1.3958174\n",
      "Loss is :  1.3955717\n",
      "Loss is :  1.3953269\n",
      "Loss is :  1.3950826\n",
      "Loss is :  1.3948393\n",
      "Loss is :  1.3945965\n",
      "Loss is :  1.3943545\n",
      "Loss is :  1.394113\n",
      "Loss is :  1.3938724\n",
      "Loss is :  1.3936325\n",
      "Loss is :  1.3933933\n",
      "Loss is :  1.3931545\n",
      "Loss is :  1.3929167\n",
      "Loss is :  1.3926795\n",
      "Loss is :  1.3924427\n",
      "Loss is :  1.392207\n",
      "Loss is :  1.3919716\n",
      "Loss is :  1.3917371\n",
      "Loss is :  1.3915032\n",
      "Loss is :  1.3912699\n",
      "Loss is :  1.3910373\n",
      "Loss is :  1.3908055\n",
      "Loss is :  1.3905743\n",
      "Loss is :  1.3903435\n",
      "Loss is :  1.3901137\n",
      "Loss is :  1.3898844\n",
      "Loss is :  1.3896558\n",
      "Loss is :  1.3894278\n",
      "Loss is :  1.3892004\n",
      "Loss is :  1.3889735\n",
      "Loss is :  1.3887476\n",
      "Loss is :  1.3885223\n",
      "Loss is :  1.3882973\n",
      "Loss is :  1.3880732\n",
      "Loss is :  1.3878496\n",
      "Loss is :  1.3876266\n",
      "Loss is :  1.3874044\n",
      "Loss is :  1.3871828\n",
      "Loss is :  1.3869618\n",
      "Loss is :  1.3867414\n",
      "Loss is :  1.3865215\n",
      "Loss is :  1.3863024\n",
      "Loss is :  1.3860838\n",
      "Loss is :  1.3858659\n",
      "Loss is :  1.3856486\n",
      "Loss is :  1.3854319\n",
      "Loss is :  1.3852158\n",
      "Loss is :  1.3850002\n",
      "Loss is :  1.3847854\n",
      "Loss is :  1.3845711\n",
      "Loss is :  1.3843575\n",
      "Loss is :  1.3841444\n",
      "Loss is :  1.3839319\n",
      "Loss is :  1.38372\n",
      "Loss is :  1.3835087\n",
      "Loss is :  1.383298\n",
      "Loss is :  1.3830878\n",
      "Loss is :  1.3828782\n",
      "Loss is :  1.3826693\n",
      "Loss is :  1.382461\n",
      "Loss is :  1.382253\n",
      "Loss is :  1.382046\n",
      "Loss is :  1.3818392\n",
      "Loss is :  1.3816332\n",
      "Loss is :  1.3814278\n",
      "Loss is :  1.3812228\n",
      "Loss is :  1.3810185\n",
      "Loss is :  1.3808146\n",
      "Loss is :  1.3806114\n",
      "Loss is :  1.3804089\n",
      "Loss is :  1.3802067\n",
      "Loss is :  1.3800051\n",
      "Loss is :  1.3798043\n",
      "Loss is :  1.3796037\n",
      "Loss is :  1.3794041\n",
      "Loss is :  1.3792046\n",
      "Loss is :  1.3790058\n",
      "Loss is :  1.3788077\n",
      "Loss is :  1.37861\n",
      "Loss is :  1.3784128\n",
      "Loss is :  1.3782164\n",
      "Loss is :  1.3780203\n",
      "Loss is :  1.3778248\n",
      "Loss is :  1.3776299\n",
      "Loss is :  1.3774354\n",
      "Loss is :  1.3772414\n",
      "Loss is :  1.3770483\n",
      "Loss is :  1.3768553\n",
      "Loss is :  1.3766631\n",
      "Loss is :  1.3764714\n",
      "Loss is :  1.3762801\n",
      "Loss is :  1.3760895\n",
      "Loss is :  1.3758992\n",
      "Loss is :  1.3757097\n",
      "Loss is :  1.3755205\n",
      "Loss is :  1.3753319\n",
      "Loss is :  1.3751439\n",
      "Loss is :  1.3749561\n",
      "Loss is :  1.3747692\n",
      "Loss is :  1.3745825\n",
      "Loss is :  1.3743966\n",
      "Loss is :  1.3742111\n",
      "Loss is :  1.3740261\n",
      "Loss is :  1.3738414\n",
      "Loss is :  1.3736575\n",
      "Loss is :  1.3734739\n",
      "Loss is :  1.3732909\n",
      "Loss is :  1.3731085\n",
      "Loss is :  1.3729265\n",
      "Loss is :  1.3727449\n",
      "Loss is :  1.372564\n",
      "Loss is :  1.3723834\n",
      "Loss is :  1.3722032\n",
      "Loss is :  1.3720237\n",
      "Loss is :  1.3718446\n",
      "Loss is :  1.3716661\n",
      "Loss is :  1.371488\n",
      "Loss is :  1.3713105\n",
      "Loss is :  1.3711332\n",
      "Loss is :  1.3709567\n",
      "Loss is :  1.3707803\n",
      "Loss is :  1.3706049\n",
      "Loss is :  1.3704296\n",
      "Loss is :  1.3702548\n",
      "Loss is :  1.3700807\n",
      "Loss is :  1.3699068\n",
      "Loss is :  1.3697335\n",
      "Loss is :  1.3695606\n",
      "Loss is :  1.3693883\n",
      "Loss is :  1.3692163\n",
      "Loss is :  1.3690449\n",
      "Loss is :  1.368874\n",
      "Loss is :  1.3687035\n",
      "Loss is :  1.3685334\n",
      "Loss is :  1.3683637\n",
      "Loss is :  1.3681946\n",
      "Loss is :  1.3680259\n",
      "Loss is :  1.3678576\n",
      "Loss is :  1.36769\n",
      "Loss is :  1.3675226\n",
      "Loss is :  1.3673557\n",
      "Loss is :  1.3671892\n",
      "Loss is :  1.3670232\n",
      "Loss is :  1.3668575\n",
      "Loss is :  1.3666927\n",
      "Loss is :  1.3665279\n",
      "Loss is :  1.3663638\n",
      "Loss is :  1.3661999\n",
      "Loss is :  1.3660365\n",
      "Loss is :  1.3658736\n",
      "Loss is :  1.3657112\n",
      "Loss is :  1.3655491\n",
      "Loss is :  1.3653877\n",
      "Loss is :  1.3652265\n",
      "Loss is :  1.3650657\n",
      "Loss is :  1.3649054\n",
      "Loss is :  1.3647455\n",
      "Loss is :  1.3645861\n",
      "Loss is :  1.364427\n",
      "Loss is :  1.3642684\n",
      "Loss is :  1.3641102\n",
      "Loss is :  1.3639526\n",
      "Loss is :  1.3637954\n",
      "Loss is :  1.3636382\n",
      "Loss is :  1.3634818\n",
      "Loss is :  1.3633257\n",
      "Loss is :  1.3631699\n",
      "Loss is :  1.3630148\n",
      "Loss is :  1.36286\n",
      "Loss is :  1.3627055\n",
      "Loss is :  1.3625516\n",
      "Loss is :  1.3623979\n",
      "Loss is :  1.3622446\n",
      "Loss is :  1.3620919\n",
      "Loss is :  1.3619394\n",
      "Loss is :  1.3617876\n",
      "Loss is :  1.3616359\n",
      "Loss is :  1.3614848\n",
      "Loss is :  1.361334\n",
      "Loss is :  1.3611835\n",
      "Loss is :  1.3610336\n",
      "Loss is :  1.360884\n",
      "Loss is :  1.3607348\n",
      "Loss is :  1.3605862\n",
      "Loss is :  1.3604376\n",
      "Loss is :  1.3602896\n",
      "Loss is :  1.360142\n",
      "Loss is :  1.3599948\n",
      "Loss is :  1.3598478\n",
      "Loss is :  1.3597014\n",
      "Loss is :  1.3595554\n",
      "Loss is :  1.3594097\n",
      "Loss is :  1.3592643\n",
      "Loss is :  1.3591194\n",
      "Loss is :  1.358975\n",
      "Loss is :  1.3588307\n",
      "Loss is :  1.358687\n",
      "Loss is :  1.3585435\n",
      "Loss is :  1.3584005\n",
      "Loss is :  1.3582579\n",
      "Loss is :  1.3581156\n",
      "Loss is :  1.3579737\n",
      "Loss is :  1.3578322\n",
      "Loss is :  1.357691\n",
      "Loss is :  1.3575503\n",
      "Loss is :  1.3574098\n",
      "Loss is :  1.3572695\n",
      "Loss is :  1.35713\n",
      "Loss is :  1.3569906\n",
      "Loss is :  1.3568516\n",
      "Loss is :  1.3567132\n",
      "Loss is :  1.3565748\n",
      "Loss is :  1.3564368\n",
      "Loss is :  1.3562994\n",
      "Loss is :  1.3561622\n",
      "Loss is :  1.3560255\n",
      "Loss is :  1.3558891\n",
      "Loss is :  1.355753\n",
      "Loss is :  1.3556173\n",
      "Loss is :  1.3554819\n",
      "Loss is :  1.3553468\n",
      "Loss is :  1.3552121\n",
      "Loss is :  1.3550777\n",
      "Loss is :  1.3549436\n",
      "Loss is :  1.3548101\n",
      "Loss is :  1.3546768\n",
      "Loss is :  1.3545439\n",
      "Loss is :  1.3544112\n",
      "Loss is :  1.3542792\n",
      "Loss is :  1.3541472\n",
      "Loss is :  1.3540156\n",
      "Loss is :  1.3538843\n",
      "Loss is :  1.3537533\n",
      "Loss is :  1.353623\n",
      "Loss is :  1.3534927\n",
      "Loss is :  1.3533628\n",
      "Loss is :  1.3532333\n",
      "Loss is :  1.353104\n",
      "Loss is :  1.3529751\n",
      "Loss is :  1.3528467\n",
      "Loss is :  1.3527184\n",
      "Loss is :  1.3525906\n",
      "Loss is :  1.3524629\n",
      "Loss is :  1.3523358\n",
      "Loss is :  1.3522089\n",
      "Loss is :  1.3520824\n",
      "Loss is :  1.3519561\n",
      "Loss is :  1.3518301\n",
      "Loss is :  1.3517046\n",
      "Loss is :  1.3515793\n",
      "Loss is :  1.3514544\n",
      "Loss is :  1.3513298\n",
      "Loss is :  1.3512056\n",
      "Loss is :  1.3510816\n",
      "Loss is :  1.3509579\n",
      "Loss is :  1.3508345\n",
      "Loss is :  1.3507115\n",
      "Loss is :  1.3505887\n",
      "Loss is :  1.3504664\n",
      "Loss is :  1.3503443\n",
      "Loss is :  1.3502225\n",
      "Loss is :  1.3501011\n",
      "Loss is :  1.3499796\n",
      "Loss is :  1.349859\n",
      "Loss is :  1.3497385\n",
      "Loss is :  1.3496181\n",
      "Loss is :  1.3494982\n",
      "Loss is :  1.3493786\n",
      "Loss is :  1.3492594\n",
      "Loss is :  1.3491404\n",
      "Loss is :  1.3490217\n",
      "Loss is :  1.3489031\n",
      "Loss is :  1.3487852\n",
      "Loss is :  1.3486674\n",
      "Loss is :  1.3485498\n",
      "Loss is :  1.3484327\n",
      "Loss is :  1.3483158\n",
      "Loss is :  1.3481991\n",
      "Loss is :  1.3480829\n",
      "Loss is :  1.3479669\n",
      "Loss is :  1.347851\n",
      "Loss is :  1.3477356\n",
      "Loss is :  1.3476206\n",
      "Loss is :  1.3475057\n",
      "Loss is :  1.3473911\n",
      "Loss is :  1.3472769\n",
      "Loss is :  1.3471631\n",
      "Loss is :  1.3470491\n",
      "Loss is :  1.346936\n",
      "Loss is :  1.3468227\n",
      "Loss is :  1.34671\n",
      "Loss is :  1.3465974\n",
      "Loss is :  1.3464851\n",
      "Loss is :  1.3463733\n",
      "Loss is :  1.3462614\n",
      "Loss is :  1.3461503\n",
      "Loss is :  1.346039\n",
      "Loss is :  1.3459283\n",
      "Loss is :  1.3458177\n",
      "Loss is :  1.3457075\n",
      "Loss is :  1.3455974\n",
      "Loss is :  1.3454876\n",
      "Loss is :  1.3453782\n",
      "Loss is :  1.3452691\n",
      "Loss is :  1.3451602\n",
      "Loss is :  1.3450516\n",
      "Loss is :  1.3449433\n",
      "Loss is :  1.344835\n",
      "Loss is :  1.3447274\n",
      "Loss is :  1.3446198\n",
      "Loss is :  1.3445127\n",
      "Loss is :  1.3444055\n",
      "Loss is :  1.3442987\n",
      "Loss is :  1.3441924\n",
      "Loss is :  1.3440863\n",
      "Loss is :  1.3439802\n",
      "Loss is :  1.3438746\n",
      "Loss is :  1.3437692\n",
      "Loss is :  1.3436639\n",
      "Loss is :  1.3435593\n",
      "Loss is :  1.3434546\n",
      "Loss is :  1.3433503\n",
      "Loss is :  1.3432462\n",
      "Loss is :  1.3431424\n",
      "Loss is :  1.3430388\n",
      "Loss is :  1.3429356\n",
      "Loss is :  1.3428326\n",
      "Loss is :  1.3427297\n",
      "Loss is :  1.3426272\n",
      "Loss is :  1.3425249\n",
      "Loss is :  1.3424227\n",
      "Loss is :  1.3423212\n",
      "Loss is :  1.3422195\n",
      "Loss is :  1.3421184\n",
      "Loss is :  1.3420173\n",
      "Loss is :  1.3419167\n",
      "Loss is :  1.3418161\n",
      "Loss is :  1.3417159\n",
      "Loss is :  1.3416158\n",
      "Loss is :  1.3415159\n",
      "Loss is :  1.3414165\n",
      "Loss is :  1.3413173\n",
      "Loss is :  1.3412182\n",
      "Loss is :  1.3411195\n",
      "Loss is :  1.341021\n",
      "Loss is :  1.3409226\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "# Loss Function\n",
    "\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_data * tf.log(prediction), reduction_indices=[1]))\n",
    "\n",
    "# Training Step\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    sess.run(train_step, feed_dict={x_data: x_train, y_data: y_train})\n",
    "    print('Loss is : ', sess.run(cross_entropy_loss, feed_dict={x_data: x_train, y_data: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9851225   0.12291209 -0.8376613   0.06456982  0.32501528]\n",
      " [ 0.19741191 -0.5658716  -2.242484   -0.01379101 -0.4817157 ]\n",
      " [-1.7173529  -1.6941004  -0.05506314 -0.23726052 -0.55309945]\n",
      " [ 0.65868753 -3.229768   -0.30366862  1.6228232   0.79802424]\n",
      " [-1.731307   -0.09106921 -0.71565187 -1.7563089  -0.86405456]\n",
      " [ 0.6670963   0.9820797   0.8369455   1.4376696   0.38501337]\n",
      " [ 0.40080607 -1.7338216   2.910719    0.35715652  1.1951851 ]\n",
      " [ 0.6131543  -0.2567973   0.6077233   1.1783066   0.5549271 ]\n",
      " [ 1.6436603   1.613455    1.0681916   0.28381637 -0.2829471 ]\n",
      " [-1.1964473  -1.3388971   0.01153098 -0.4194493  -1.2240324 ]\n",
      " [-0.7099848   0.18468183  1.8527306   0.07417105  1.7195776 ]]\n",
      "\n",
      "\n",
      "************\n",
      "\n",
      "[-0.8675415   0.40560988 -0.74447423 -0.7293968   1.744267  ]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(w1))\n",
    "print('\\n\\n************\\n')\n",
    "print(sess.run(b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.852664  ,  0.52852196, -1.5821356 , -0.664827  ,  2.0692823 ],\n",
       "       [-0.6701296 , -0.16026172, -2.9869583 , -0.74318784,  1.2625513 ],\n",
       "       [-2.5848944 , -1.2884905 , -0.79953736, -0.96665734,  1.1911676 ],\n",
       "       [-0.20885396, -2.8241582 , -1.0481429 ,  0.8934264 ,  2.5422912 ],\n",
       "       [-2.5988486 ,  0.31454068, -1.4601262 , -2.4857059 ,  0.8802124 ],\n",
       "       [-0.20044518,  1.3876896 ,  0.09247124,  0.7082728 ,  2.1292803 ],\n",
       "       [-0.46673542, -1.3282118 ,  2.1662447 , -0.3722403 ,  2.9394522 ],\n",
       "       [-0.2543872 ,  0.14881256, -0.13675094,  0.44890976,  2.299194  ],\n",
       "       [ 0.7761188 ,  2.019065  ,  0.32371742, -0.44558045,  1.4613199 ],\n",
       "       [-2.0639887 , -0.93328726, -0.73294324, -1.1488461 ,  0.5202346 ],\n",
       "       [-1.5775263 ,  0.59029174,  1.1082563 , -0.65522575,  3.4638445 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = sess.run(w1+b1)\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5848944  -1.2884905  -0.79953736 -0.96665734  1.1911676 ]\n"
     ]
    }
   ],
   "source": [
    "print(vec[word2int['india'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Average word2vec for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vectors = []\n",
    "\n",
    "for sent in sentences:\n",
    "    sent_vect = np.zeros(5)\n",
    "    cnt = 0\n",
    "    for index,word in enumerate(sent):\n",
    "        word_vect = vec[word2int[word]]\n",
    "        sent_vect += word_vect\n",
    "        cnt+=1\n",
    "    sent_vect /= cnt\n",
    "    sent_vectors.append(sent_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.58307811, -0.68596956, -1.57538005, -0.79339032,  1.58910096]),\n",
       " array([-1.47889696, -0.61492891, -1.56206123, -0.82982808,  1.45491436]),\n",
       " array([-0.33548841, -0.03106475,  0.52850937,  0.02573054,  2.50723763]),\n",
       " array([-0.34627682, -0.27884015,  0.48266493, -0.02614207,  2.54122038])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.58307811, -0.68596956, -1.57538005, -0.79339032,  1.58910096])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1., -1.],\n",
       "       [ 1.,  1.,  1.,  1., -1.],\n",
       "       [ 1.,  1.,  1.,  1., -1.],\n",
       "       [ 1.,  1.,  1.,  1., -1.],\n",
       "       [-1., -1., -1., -1.,  1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sent_vectors[0].reshape(-1, 1),sent_vectors[1].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity between Sentence 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990372628875064"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "sim1 = 1 - spatial.distance.cosine(sent_vectors[0], sent_vectors[1])\n",
    "sim1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity between Sentence 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9951470596701174"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim2 = 1 - spatial.distance.cosine(sent_vectors[2], sent_vectors[3])\n",
    "sim2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see here that as the two sentences \"I am going to India\" and \"I am going to Bharat\" differ only by one word the cosine similarity score of the two sentences is very close to one.**\n",
    "\n",
    "**Similarly the cosine similarity score of the two sentences \"I will be eating coffee\" and \"I will be drinking coffee\" are also close to one as they also differ only by one word.**\n",
    "\n",
    "**If the text corpus is large, the word embeddings will be even more accurate and the similarity scores will improve for new sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Implementation of Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences,min_count=1,size=5,window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'india',\n",
       " 'bharat',\n",
       " 'will',\n",
       " 'be',\n",
       " 'eating',\n",
       " 'coffee',\n",
       " 'drinking']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Average word2vec for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vectors_w2v = []\n",
    "\n",
    "for sent in sentences:\n",
    "    sent_vect = np.zeros(5)\n",
    "    cnt = 0\n",
    "    for index,word in enumerate(sent):\n",
    "        word_vect = w2v_model.wv[word]\n",
    "        sent_vect += word_vect\n",
    "        cnt+=1\n",
    "    sent_vect /= cnt\n",
    "    sent_vectors_w2v.append(sent_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.00962927, -0.03535876,  0.0057662 ,  0.02018384, -0.02800476]),\n",
       " array([ 0.00940491, -0.0367009 ,  0.01115323,  0.0024141 , -0.03248497]),\n",
       " array([ 0.00510323, -0.01051808,  0.01030579,  0.00431587, -0.01589096]),\n",
       " array([-0.01288132, -0.03627979,  0.01526736, -0.01725841, -0.01075622])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vectors_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  1.,  1., -1.],\n",
       "       [-1.,  1., -1., -1.,  1.],\n",
       "       [ 1., -1.,  1.,  1., -1.],\n",
       "       [ 1., -1.,  1.,  1., -1.],\n",
       "       [-1.,  1., -1., -1.,  1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sent_vectors_w2v[0].reshape(-1, 1),sent_vectors_w2v[1].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9293752410251761"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = 1 - spatial.distance.cosine(sent_vectors_w2v[0], sent_vectors_w2v[1])\n",
    "a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we can see that the cosine similarity score is lesser than the custom implementation but if the text corpus size is increased, the Gensim Word2Vec model tends to learn better word embeddings and gives good similarity scores. **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
